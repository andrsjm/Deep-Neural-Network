{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e184fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import learning_curve\n",
    "from math import sqrt\n",
    "from numpy.random import randn\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fefa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cardio_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd110be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff1b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def label_encoding(self, columns):\n",
    "        label_encoder = LabelEncoder()\n",
    "        for column in columns:\n",
    "            self.df[column]= label_encoder.fit_transform(self.df[column])\n",
    "            self.df[column].unique()\n",
    "    \n",
    "    def clean_outliers(self, columns):\n",
    "        for column in columns:   \n",
    "            q1, q3 = self.df[column].quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.df = self.df[(self.df[column] <= upper_bound )]\n",
    "            self.df = self.df[(self.df[column] >= lower_bound )]\n",
    "    \n",
    "    def missing_value(self):\n",
    "        self.df=self.df.dropna()\n",
    "        \n",
    "    def duplicated_data(self):\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def set_df(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def min_max_scaller(self, X):\n",
    "        # define min max scaler\n",
    "        scaler = MinMaxScaler()\n",
    "        # transform data\n",
    "        scaled = scaler.fit_transform(X)\n",
    "        return scaled\n",
    "    \n",
    "    def split_dataset(self, X, y, test_size=0.2, random_state=42):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc7d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7475ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.duplicated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0ab6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.clean_outliers(['height', 'weight', 'ap_lo', 'ap_hi', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8664b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.missing_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40242f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pp.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc85020",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'], axis='column')\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f1720b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wadidaw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8160\\2315058665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwadidaw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wadidaw' is not defined"
     ]
    }
   ],
   "source": [
    "wadidaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f21f12",
   "metadata": {},
   "source": [
    "## Data Splitting & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aade8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pp.split_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9018ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pp.min_max_scaller(X_train)\n",
    "X_test_scaled = pp.min_max_scaller(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac30bf",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85be961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n",
    "def sigmoid_act(x, der=False):\n",
    "    import numpy as np\n",
    "    \n",
    "    if (der==True) : #derivative of the sigmoid\n",
    "        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n",
    "    else : # sigmoid\n",
    "        f = 1/(1+ np.exp(- x))\n",
    "    \n",
    "    return f\n",
    "\n",
    "# We may employ the Rectifier Linear Unit (ReLU)\n",
    "def ReLU_act(x, der=False):\n",
    "    import numpy as np\n",
    "    \n",
    "    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n",
    "        f = np.heaviside(x, 1)\n",
    "    else :\n",
    "        f = np.maximum(x, 0)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7adb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Artificial Neural Network Class\n",
    "'''\n",
    "class DeepNeuralNetwork:\n",
    "    import numpy as np # linear algebra\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    '''\n",
    "    Initialize the DNN;\n",
    "    HiddenLayer vector : will contain the Layers' info\n",
    "    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n",
    "    mu = cost function\n",
    "    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n",
    "    '''\n",
    "    def __init__(self, encoder, scaler) :\n",
    "        self.HiddenLayer = []\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        self.phi = []\n",
    "        self.mu = []\n",
    "        self.eta = 1 #set up the proper Learning Rate!!\n",
    "        self.encoder = encoder\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    '''\n",
    "    add method: to add layers to the network\n",
    "    '''\n",
    "    def add(self, lay = (4, 'ReLU') ):\n",
    "        self.HiddenLayer.append(lay)\n",
    "    \n",
    "    '''\n",
    "    FeedForward method: as explained before. \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def FeedForward(w, b, phi, x):\n",
    "        return phi(np.dot(w, x) + b)\n",
    "        \n",
    "    '''\n",
    "    BackPropagation algorithm implementing the Gradient Descent \n",
    "    '''\n",
    "    def BackPropagation(self, x, z, Y, w, b, phi):\n",
    "        self.delta = []\n",
    "        \n",
    "        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        \n",
    "        # We start computing the LAST error, the one for the OutPut Layer \n",
    "        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n",
    "        \n",
    "        '''Now we BACKpropagate'''\n",
    "        # We thus compute from next-to-last to first\n",
    "        for i in range(0, len(z)-1):\n",
    "            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n",
    "        \n",
    "        # We have the error array ordered from last to first; we flip it to order it from first to last\n",
    "        temp_delta = []\n",
    "        for i in reversed(self.delta):\n",
    "            temp_delta.append(i)\n",
    "            \n",
    "        self.delta = np.array(temp_delta, dtype=object)\n",
    "        \n",
    "        # Now we define the delta as the error divided by the number of training samples\n",
    "        self.delta = self.delta/self.X.shape[0] \n",
    "        \n",
    "        '''GRADIENT DESCENT'''\n",
    "        # We start from the first layer that is special, since it is connected to the Input Layer\n",
    "        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n",
    "        self.B.append( b[0] - self.eta * self.delta[0] )\n",
    "        \n",
    "        # We now descend for all the other Hidden Layers + OutPut Layer\n",
    "        for i in range(1, len(z)):\n",
    "            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n",
    "            self.B.append( b[i] - self.eta * self.delta[i] )\n",
    "        \n",
    "        # We return the descended parameters w, b\n",
    "        np.warnings.filterwarnings('error', category=np.VisibleDeprecationWarning)\n",
    "        return np.array(self.W, dtype=object), np.array(self.B, dtype=object)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Fit method: it calls FeedForward and Backpropagation methods\n",
    "    '''\n",
    "    def Fit(self, X_train, Y_train, epochs):            \n",
    "        print('Start fitting...')\n",
    "        '''\n",
    "        Input layer\n",
    "        '''\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        accurations= []\n",
    "        recalls = []\n",
    "        \n",
    "        '''\n",
    "        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n",
    "        '''\n",
    "        print('Model recap: \\n')\n",
    "        print('You are fitting an DNN with the following amount of layers: ', len(self.HiddenLayer))\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(self.HiddenLayer)) :\n",
    "            \n",
    "            print('Layer ', i+1)\n",
    "            print('Number of neurons: ', self.HiddenLayer[i][0])\n",
    "            if i==0:\n",
    "                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n",
    "                # Old initialization\n",
    "                #self.w.append(2 * np.random.rand(self.HiddenLayer[i][0] , self.X.shape[1]) - 0.5)\n",
    "                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n",
    "            \n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "            \n",
    "            else :\n",
    "                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "                # Old initialization\n",
    "                #self.w.append(2*np.random.rand(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] ) - 0.5)\n",
    "                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n",
    "                \n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "            \n",
    "        '''\n",
    "        Now we start the Loop over the training dataset\n",
    "        '''  \n",
    "        for epoch in range (0, epochs):\n",
    "            print(\"Epoch : \", epoch + 1)\n",
    "            for I in range(0, self.X.shape[0]): # loop over the training set\n",
    "                '''\n",
    "                Now we start the feed forward\n",
    "                '''  \n",
    "                self.z = []\n",
    "\n",
    "                self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n",
    "\n",
    "                for i in range(1, len(self.HiddenLayer)): #Looping over layers\n",
    "                    self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n",
    "\n",
    "\n",
    "                '''\n",
    "                Here we backpropagate\n",
    "                '''      \n",
    "                self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y.iloc[I], self.w, self.b, self.phi)\n",
    "\n",
    "                '''\n",
    "                Compute cost function\n",
    "                ''' \n",
    "                self.mu.append(\n",
    "                    (1/2) * np.dot(self.z[len(self.z)-1] - self.Y.iloc[I], self.z[len(self.z)-1] - self.Y.iloc[I]) \n",
    "                )\n",
    "                \n",
    "            prediction = self.get_accuration(X_train)\n",
    "            prediction[np.isnan(prediction)] = 0\n",
    "            print(\"Accuracy : \", accuracy_score(y_train, prediction))\n",
    "            accurations.append(accuracy_score(y_train, prediction))\n",
    "            recalls.append(recall_score(y_train, prediction))\n",
    "            \n",
    "        return accurations, recalls\n",
    "            \n",
    "        print('Fit done. \\n')\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    predict method\n",
    "    '''\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        print('Starting predictions...')\n",
    "        \n",
    "        self.pred = []\n",
    "        self.XX = X_test\n",
    "        \n",
    "        for I in range(0, self.XX.shape[0]): # loop over the training set\n",
    "            \n",
    "            '''\n",
    "            Now we start the feed forward\n",
    "            '''  \n",
    "            self.z = []\n",
    "            \n",
    "            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n",
    "    \n",
    "            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n",
    "                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n",
    "       \n",
    "            # Append the prediction;\n",
    "            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n",
    "            # if y < 0.5 the output is zero, otherwise is zero\n",
    "            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n",
    "        \n",
    "        print('Predictions done. \\n')\n",
    "\n",
    "        return np.array(self.pred)\n",
    "    \n",
    "    \n",
    "    def get_accuration(self, X):\n",
    "        self.pred = []\n",
    "        self.XX = X\n",
    "        \n",
    "        for I in range(0, self.XX.shape[0]): # loop over the training set\n",
    "            \n",
    "            '''\n",
    "            Now we start the feed forward\n",
    "            '''  \n",
    "            self.z = []\n",
    "            \n",
    "            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n",
    "    \n",
    "            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n",
    "                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n",
    "       \n",
    "            # Append the prediction;\n",
    "            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n",
    "            # if y < 0.5 the output is zero, otherwise is zero\n",
    "            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n",
    "\n",
    "        return np.array(self.pred)\n",
    "        \n",
    "    def encode_data(self, data, column):\n",
    "        list_encode =  self.encoder[column].transform([data])\n",
    "        return list_encode[0]\n",
    "    \n",
    "    def scale_data(self, data):\n",
    "        return self.scaler.transform([data])\n",
    "    '''\n",
    "    We need a method to retrieve the accuracy for each training data to follow the learning of the DNN\n",
    "    '''\n",
    "    def get_accuracy(self):\n",
    "        return np.array(self.mu)\n",
    "    # This is the averaged version\n",
    "    def get_avg_accuracy(self):\n",
    "        import math\n",
    "        self.batch_loss = []\n",
    "        for i in range(0, 10):\n",
    "            self.loss_avg = 0\n",
    "            # To set the batch in 10 element/batch we use math.ceil method\n",
    "            # int(math.ceil((self.X.shape[0]-10) / 10.0))    - 1\n",
    "            for m in range(0, (int(math.ceil((self.X.shape[0]-10) / 10.0))   )-1):\n",
    "                #self.loss_avg += self.mu[60*i+m]/60\n",
    "                self.loss_avg += self.mu[(int(math.ceil((self.X.shape[0]-10) / 10.0)) )*i + m]/(int(math.ceil((self.X.shape[0]-10) / 10.0)) )\n",
    "            self.batch_loss.append(self.loss_avg)\n",
    "        return np.array(self.batch_loss)\n",
    "    \n",
    "    '''\n",
    "    Method to set the learning rate\n",
    "    '''\n",
    "    def set_learning_rate(self, et=1):\n",
    "        self.eta = et\n",
    "        \n",
    "        \n",
    "'''\n",
    "layers class\n",
    "'''\n",
    "class layers :\n",
    "    '''\n",
    "    Layer method: used to call standar layers to add. \n",
    "    Easily generalizable to more general layers (Pooling and Convolutional layers)\n",
    "    '''        \n",
    "    def layer(p=4, activation = 'ReLU'):\n",
    "        return (p, activation)\n",
    "\n",
    "'''\n",
    "Activation functions class\n",
    "'''\n",
    "class Activation_function(DeepNeuralNetwork):\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        \n",
    "    '''\n",
    "    Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n",
    "    '''\n",
    "    def sigmoid_act(x, der=False):\n",
    "        if (der==True) : #derivative of the sigmoid\n",
    "            f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n",
    "        else : # sigmoid\n",
    "            f = 1/(1+ np.exp(- x))\n",
    "        return f\n",
    "\n",
    "    '''\n",
    "    Define the Rectifier Linear Unit (ReLU)\n",
    "    '''\n",
    "    def ReLU_act(x, der=False):\n",
    "        if (der == True): # the derivative of the ReLU is the Heaviside Theta\n",
    "            f = np.heaviside(x, 1)\n",
    "        else :\n",
    "            f = np.maximum(x, 0)\n",
    "        return f\n",
    "    \n",
    "    def list_act():\n",
    "        return ['sigmoid', 'ReLU']\n",
    "    \n",
    "    def get_act(string = 'ReLU'):\n",
    "        if string == 'ReLU':\n",
    "            return ReLU_act\n",
    "        elif string == 'sigmoid':\n",
    "            return sigmoid_act\n",
    "        else :\n",
    "            return sigmoid_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04f7ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50001, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "812a79ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting...\n",
      "Model recap: \n",
      "\n",
      "You are fitting an DNN with the following amount of layers:  5\n",
      "Layer  1\n",
      "Number of neurons:  11\n",
      "\tActivation:  ReLU\n",
      "Layer  2\n",
      "Number of neurons:  11\n",
      "\tActivation:  ReLU\n",
      "Layer  3\n",
      "Number of neurons:  11\n",
      "\tActivation:  ReLU\n",
      "Layer  4\n",
      "Number of neurons:  11\n",
      "\tActivation:  ReLU\n",
      "Layer  5\n",
      "Number of neurons:  1\n",
      "\tActivation:  sigmoid\n",
      "Epoch :  1\n",
      "Accuracy :  0.4879702405951881\n",
      "Epoch :  2\n",
      "Accuracy :  0.4743905121897562\n",
      "Epoch :  3\n",
      "Accuracy :  0.4925101497970041\n",
      "Epoch :  4\n",
      "Accuracy :  0.5260094798104038\n",
      "Epoch :  5\n",
      "Accuracy :  0.5376492470150597\n",
      "Epoch :  6\n",
      "Accuracy :  0.5506289874202516\n",
      "Epoch :  7\n",
      "Accuracy :  0.5578288434231315\n",
      "Epoch :  8\n",
      "Accuracy :  0.5626487470250595\n",
      "Epoch :  9\n",
      "Accuracy :  0.5657286854262915\n",
      "Epoch :  10\n",
      "Accuracy :  0.5685686286274274\n",
      "Epoch :  11\n",
      "Accuracy :  0.5723285534289314\n",
      "Epoch :  12\n",
      "Accuracy :  0.5744085118297634\n",
      "Epoch :  13\n",
      "Accuracy :  0.5751484970300594\n",
      "Epoch :  14\n",
      "Accuracy :  0.5756884862302754\n",
      "Epoch :  15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8160\\1981659565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mstrings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'epoch '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m', lr '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m', rs '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mdict_acc_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_rec_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0macc_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8160\\1561348227.py\u001b[0m in \u001b[0;36mFit\u001b[1;34m(self, X_train, Y_train, epochs)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[0mHere\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mbackpropagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 '''      \n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 '''\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8160\\1561348227.py\u001b[0m in \u001b[0;36mBackPropagation\u001b[1;34m(self, x, z, Y, w, b, phi)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# We now descend for all the other Hidden Layers + OutPut Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mkron\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mkron\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m     \u001b[0mwrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_array_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_acc_train = {}\n",
    "dict_acc_test = {}\n",
    "dict_rec_train = {}\n",
    "dict_rec_test = {}\n",
    "\n",
    "epoch = [150]\n",
    "learning_rate = [0.01]\n",
    "random_state = [42]\n",
    "\n",
    "\n",
    "\n",
    "for i in epoch:\n",
    "    for j in learning_rate:\n",
    "        for k in random_state:                                 \n",
    "            model = DeepNeuralNetwork(None, None)\n",
    "            model.add(layers.layer(11, 'ReLU'))\n",
    "            model.add(layers.layer(11, 'ReLU'))\n",
    "            model.add(layers.layer(11, 'ReLU'))\n",
    "            model.add(layers.layer(11, 'ReLU'))\n",
    "#             model.add(layers.layer(11, 'ReLU'))\n",
    "            model.add(layers.layer(1, 'sigmoid'))\n",
    "\n",
    "            model.set_learning_rate(j)\n",
    "\n",
    "            epoch = i\n",
    "\n",
    "            strings = 'epoch '+str(i)+', lr '+str(j)+', rs '+str(k)\n",
    "\n",
    "            dict_acc_train[strings], dict_rec_train[strings] = model.Fit(X_train_scaled, y_train, epoch)\n",
    "            \n",
    "            acc_val = model.get_accuracy()\n",
    "            acc_avg_val = model.get_avg_accuracy()\n",
    "\n",
    "            predictions_test = model.predict(X_test_scaled)\n",
    "            dict_acc_test[strings] = accuracy_score(y_test, predictions_test)\n",
    "            dict_rec_test[strings] = recall_score(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ae0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5da600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d340f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_rec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "wads = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = list(range(0, wads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, dict_acc_train['epoch '+ str(wads) +', lr 0.01, rs 42'], label = \"akurasi\")\n",
    "plt.plot(x_axis, dict_rec_train['epoch '+ str(wads) +', lr 0.01, rs 42'], label = \"recall\")\n",
    "plt.xlabel('epoch')\n",
    "# plt.ylabel('recall')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
